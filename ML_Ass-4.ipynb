{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cac7ff9-34d5-4682-b762-c21576af1004",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88b118-4708-43e6-9281-0e6e1ba45708",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in a dataset refer to the absence of values in some observations or variables. Missing values can occur for various reasons, \n",
    "such as human error, data corruption, or system failure. Missing values can lead to inaccurate or biased results if not handled appropriately.\n",
    "\n",
    "It is essential to handle missing values in a dataset as they can affect the accuracy and reliability of the models built on that dataset. \n",
    "Missing values can also reduce the sample size, which can affect the power of the analysis. Furthermore, some machine learning algorithms \n",
    "cannot handle missing values, while others require handling missing values explicitly.\n",
    "\n",
    "Algorithms that are not affected by missing values include tree-based models such as decision trees, random forests, and gradient boosting\n",
    "machines, as they can handle missing values without imputation. Naive Bayes is another algorithm that can handle missing values as it \n",
    "estimates the likelihood of a feature value given a class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db2b07-b378-4380-9735-63b9c99baf79",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2698c-a105-412d-89bf-6d93d7b5ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques to handle missing data, including:\n",
    "\n",
    "Deletion: Delete the rows or columns with missing data.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Delete rows with missing values\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Delete columns with missing values\n",
    "df.dropna(axis=1, inplace=True)\n",
    "\n",
    "\n",
    "Imputation: Replace missing values with estimated values.\n",
    "\n",
    "\n",
    "# Imputing missing values using mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Imputing missing values using median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Imputing missing values using mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "\n",
    "Mean, median or mode imputation: Replace missing values with the mean, median, or mode value of the feature.\n",
    "\n",
    "\n",
    "# Imputing missing values using mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Imputing missing values using median\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Imputing missing values using mode\n",
    "df.fillna(df.mode().iloc[0], inplace=True)\n",
    "\n",
    "\n",
    "Interpolation: Estimate the missing values using the values of the other observations.\n",
    "\n",
    "\n",
    "# Interpolate missing values\n",
    "df.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "\n",
    "\n",
    "K-Nearest Neighbors (KNN): Estimate the missing values using the values of the k-nearest neighbors.\n",
    "\n",
    "\n",
    "# Imputing missing values using KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "\n",
    "Multiple Imputation: Create multiple imputations for missing values and combine the results.\n",
    "\n",
    "\n",
    "# Multiple imputation using MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "\n",
    "These are some commonly used techniques for handling missing data. It is essential to choose the appropriate technique based on the \n",
    "type and extent of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e98bc-491d-4cc5-bf8a-b21c53c6b9f0",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c2e21e-1c89-401b-9479-d003b28007c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data is a situation where the distribution of the target variable in the dataset is skewed towards one class, \n",
    "resulting in a significant difference between the number of samples in the majority class and the minority class. For instance, \n",
    "in a binary classification problem where the positive class (fraudulent transactions) constitutes only 1% of the entire dataset, \n",
    "and the negative class (non-fraudulent transactions) constitutes 99%, then the dataset is considered imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can result in biased machine learning models that tend to predict the majority class more accurately\n",
    "and ignore the minority class. This can be a significant problem in many real-world scenarios, such as fraud detection, disease diagnosis,\n",
    "and customer churn prediction, where the minority class is of more interest.\n",
    "\n",
    "Some of the consequences of not handling imbalanced data include:\n",
    "\n",
    "Poor performance of the machine learning model for the minority class\n",
    "High false negative rate (Type II error)\n",
    "Overfitting to the majority class\n",
    "Misleading evaluation metrics, such as accuracy, which can be high due to the large number of negative instances\n",
    "There are several techniques used to handle imbalanced data, including:\n",
    "\n",
    "Random Under-Sampling (RUS): This technique involves randomly selecting a subset of samples from the majority class to balance the class distribution.\n",
    "Example in Python:\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "\n",
    "Random Over-Sampling (ROS): This technique involves creating synthetic samples from the minority class to balance the class distribution.\n",
    "Example in Python:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "\n",
    "Synthetic Minority Over-Sampling Technique (SMOTE): This technique involves creating synthetic samples from the minority class by\n",
    "interpolating between existing samples.\n",
    "Example in Python:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "Ensemble techniques: Ensemble techniques, such as Bagging and Boosting, can be used to improve the performance of imbalanced datasets. \n",
    "These techniques involve training multiple models on different subsets of the data and combining their predictions.\n",
    "Example in Python:\n",
    "\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "brf = BalancedRandomForestClassifier(random_state=42)\n",
    "brf.fit(X_train, y_train)\n",
    "y_pred = brf.predict(X_test)\n",
    "\n",
    "\n",
    "Cost-sensitive learning: This technique involves assigning different costs to misclassification errors of different classes to account for \n",
    "the imbalanced nature of the dataset.\n",
    "Example in Python:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear', class_weight='balanced')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449886c3-0cfe-4e55-9a50-f8cc9c7e0d41",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a051bc7-ae40-4748-be35-491ed5e9c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Up-sampling and down-sampling are techniques used in data preprocessing to address the issue of imbalanced datasets.\n",
    "\n",
    "Up-sampling is a technique used to increase the number of samples in the minority class by randomly replicating the existing samples. \n",
    "This is done until the minority class is balanced with the majority class.\n",
    "Down-sampling, on the other hand, is a technique used to reduce the number of samples in the majority class by randomly removing samples.\n",
    "This is done until the majority class is balanced with the minority class.\n",
    "Here's an example when up-sampling and down-sampling are required:\n",
    "\n",
    "Suppose we have a dataset of 1000 samples, out of which only 100 samples belong to the minority class, and the rest belong to the majority \n",
    "class. In this case, the dataset is imbalanced, and we need to balance the dataset to avoid biased machine learning models.\n",
    "\n",
    "If we have a sufficient amount of data, we can down-sample the majority class by randomly removing 900 samples to balance the dataset. \n",
    "However, if we do not have enough data, we can up-sample the minority class by randomly replicating the existing 100 samples until we \n",
    "have 900 samples for the minority class, thereby balancing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e772c-77d8-4672-98b0-fe487fee274d",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bbf2a-daf6-49d5-852d-046fb09148b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by applying different transformations to existing data, \n",
    "resulting in new synthetic data points. It is particularly useful when working with limited data, and it helps improve the performance \n",
    "and generalization of machine learning models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation method used to handle imbalanced datasets. \n",
    "It involves synthesizing new instances of the minority class by interpolating between existing instances. SMOTE selects a random \n",
    "minority sample and then selects its k nearest neighbors to create new synthetic instances in between them. The number of synthetic\n",
    "instances generated is based on the imbalance ratio between the minority and majority classes.\n",
    "\n",
    "Here is an example of using SMOTE in Python:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# create SMOTE object\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# fit and transform the data\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "In this example, we first import the SMOTE module from the imblearn library. We then create a SMOTE object with a random state of 42.\n",
    "Finally, we fit and transform the training data using the fit_resample() method to generate new synthetic data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655150a0-0495-4866-952e-af60ddf04e51",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c96868-3bdb-49f8-b7e7-8dc24dae9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers are data points in a dataset that deviate significantly from other data points. Outliers can occur due to measurement or \n",
    "input errors, or they may represent genuine but extreme values in the data. It is essential to handle outliers because they can \n",
    "significantly affect the results of data analysis and machine learning models. Outliers can distort statistical analyses, reduce\n",
    "the accuracy of predictive models, and impact decision-making.\n",
    "\n",
    "There are several ways to handle outliers in a dataset:\n",
    "\n",
    "Removal: One approach is to remove the outlier data points from the dataset. However, this approach should be taken with caution as \n",
    "removing too many outliers can significantly reduce the size of the dataset, which can have a negative impact on model performance.\n",
    "\n",
    "Capping: Another approach is to cap the outliers by replacing them with the nearest acceptable values. For example, if the data point\n",
    "is too high, we can replace it with the maximum value that is acceptable.\n",
    "\n",
    "Transformation: We can transform the data using techniques like log transformation, z-score normalization, or box-cox transformation to \n",
    "reduce the impact of outliers on the model.\n",
    "\n",
    "Model-based methods: Another approach is to use model-based methods that are robust to outliers. For example, decision tree-based models, \n",
    "such as Random Forest, are robust to outliers as they are based on splitting the data into regions.\n",
    "\n",
    "SMOTE stands for Synthetic Minority Over-sampling Technique, which is a technique used to address imbalanced datasets by generating \n",
    "synthetic samples for the minority class. It creates new samples by interpolating between existing samples of the minority class. \n",
    "SMOTE is a popular technique used in machine learning to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4f75b-d5bf-41d5-bd73-d00fc7e22775",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8962cd-6947-4549-b3c9-c87c06d0e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset. Some of them are:\n",
    "\n",
    "Deletion: In this method, rows or columns with missing values are deleted from the dataset. This method can be further classified \n",
    "into three categories:\n",
    "\n",
    "a. Listwise deletion or complete case analysis: In this method, any row with missing values is removed from the dataset.\n",
    "\n",
    "b. Pairwise deletion or available case analysis: In this method, only those rows with complete data for a given set of variables are used.\n",
    "\n",
    "c. Column-wise deletion: In this method, columns with a high percentage of missing values are removed.\n",
    "\n",
    "Mean/median imputation: In this method, missing values are replaced with the mean or median of the available data for that feature.\n",
    "\n",
    "Mode imputation: In this method, missing categorical data is replaced with the mode or most common category in the available data.\n",
    "\n",
    "Regression imputation: In this method, the missing values of a feature are predicted using a regression model built with other \n",
    "features in the dataset.\n",
    "\n",
    "K-nearest neighbor imputation: In this method, the missing values of a feature are predicted using the values of its k-nearest neighbors.\n",
    "\n",
    "Multiple imputation: In this method, multiple imputed datasets are created, and the analysis is performed on each dataset, and the results \n",
    "are combined.\n",
    "\n",
    "For example, in Python, we can use the Pandas library to handle missing data. Here's how we can perform mean imputation on a dataframe\n",
    "using the fillna() function:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# creating a sample dataframe with missing values\n",
    "data = {'A': [1, 2, np.nan, 4, 5], 'B': [np.nan, 7, 8, 9, 10], 'C': [11, 12, 13, np.nan, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# performing mean imputation on the dataframe\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df)\n",
    "\n",
    "This will replace the missing values in the dataframe with the mean of the available data for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9805aa8-2836-4707-88a0-4aafd2ab047b",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a8a67-3a31-429a-841d-f1bcf5878a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are various strategies that can be used to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data. Here are some of them:\n",
    "\n",
    "Visualization: Visualization techniques such as scatter plots, box plots, and histograms can be used to visualize the distribution \n",
    "of the data and identify if there is a pattern to the missing data.\n",
    "\n",
    "Correlation analysis: Correlation analysis can be used to identify if there is a correlation between the missing data and other variables \n",
    "in the dataset. If there is a correlation, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Hypothesis testing: Hypothesis testing can be used to test if there is a significant difference between the values of the missing data \n",
    "and the values of the rest of the data. If there is a significant difference, it may indicate that the missing data is not missing at random.\n",
    "\n",
    "Imputation techniques: Imputation techniques can be used to fill in the missing data and determine if the imputed values are significantly \n",
    "different from the rest of the data. If the imputed values are significantly different, it may indicate that the missing data is not missing \n",
    "at random.\n",
    "\n",
    "Overall, it is important to carefully analyze the missing data and consider multiple strategies to determine if the missing data is missing \n",
    "at random or if there is a pattern to the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527fd2c8-0420-4e0f-8271-04c072b4566a",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d569e-fe42-4bc4-aa85-240c731cd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several strategies you can use to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "Confusion matrix: A confusion matrix is a table that compares the predicted values of a model with the actual values. \n",
    "It provides information about true positive, true negative, false positive, and false negative rates. It is a useful tool for \n",
    "evaluating the performance of a model on imbalanced data.\n",
    "\n",
    "Precision and Recall: Precision and Recall are two important metrics for evaluating the performance of a model on imbalanced data. \n",
    "Precision measures the proportion of correctly identified positive cases out of all predicted positive cases, while recall measures\n",
    "the proportion of correctly identified positive cases out of all actual positive cases.\n",
    "\n",
    "F1-score: The F1-score is a harmonic mean of precision and recall, which gives equal weight to both metrics. It is a useful measure \n",
    "for evaluating the overall performance of a model on imbalanced data.\n",
    "\n",
    "ROC curve and AUC: The Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive \n",
    "rate for different classification thresholds. The Area Under the Curve (AUC) is a metric that measures the overall performance of a \n",
    "model in distinguishing between positive and negative classes.\n",
    "\n",
    "Resampling techniques: Resampling techniques such as oversampling, undersampling, and SMOTE can also be used to balance the dataset \n",
    "before training the model.\n",
    "\n",
    "Class weighting: In some models, it is possible to assign weights to different classes to reflect their imbalance in the dataset. \n",
    "This can help the model to focus more on the minority class and improve its performance on imbalanced data.\n",
    "\n",
    "It is important to note that there is no single best strategy for evaluating the performance of a model on imbalanced data. The most \n",
    "appropriate strategy will depend on the specific dataset and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07952d4-c8fb-4e0d-bd69-eab82ab0ae55",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b354d5-0e0b-4e06-b283-106252a9a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with an imbalanced dataset, there are various methods to balance the dataset and down-sample the majority class,\n",
    "such as:\n",
    "\n",
    "Undersampling: In this technique, the majority class is down-sampled to match the number of instances in the minority class. \n",
    "Random sampling can be used to select the subset of the majority class to be retained.\n",
    "\n",
    "Oversampling: In this technique, the minority class is over-sampled to match the number of instances in the majority class. \n",
    "This can be done through techniques like SMOTE (Synthetic Minority Over-sampling Technique), which creates synthetic samples by \n",
    "interpolating between existing minority samples.\n",
    "\n",
    "Hybrid sampling: This technique involves a combination of undersampling and oversampling techniques to balance the dataset.\n",
    "\n",
    "Here is an example of using the SMOTE technique in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "In this code, X and y represent the feature matrix and target vector, respectively. The SMOTE function is called from the imblearn package,\n",
    "and the fit_resample method is used to generate the new, balanced dataset. The resulting X_resampled and y_resampled arrays can then be used \n",
    "for further analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8431a-8c66-4eee-8555-ad653f86dc86",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90da28-a56c-49fa-bf3d-82b0ad3ac853",
   "metadata": {},
   "outputs": [],
   "source": [
    "In cases where the dataset is unbalanced with a low percentage of occurrences of a particular class, we can use the following \n",
    "techniques to balance the dataset and up-sample the minority class:\n",
    "\n",
    "Random Oversampling: This technique involves randomly duplicating samples from the minority class to balance the dataset.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This technique involves generating synthetic data points for the minority class \n",
    "based on the existing data points. The synthetic data points are generated by creating new data points along the line segments between\n",
    "existing data points.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): This technique is similar to SMOTE, but it generates synthetic data points based on the density\n",
    "of the minority class.\n",
    "\n",
    "Here's an example of how to use SMOTE for up-sampling the minority class in Python using the imbalanced-learn library:\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate SMOTE object\n",
    "sm = SMOTE()\n",
    "\n",
    "# Upsample the minority class\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "In the above code, X is the feature matrix and y is the target vector. The fit_resample() method of the SMOTE object performs the \n",
    "SMOTE algorithm to up-sample the minority class. The output is the up-sampled feature matrix X_resampled and target vector y_resampled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
